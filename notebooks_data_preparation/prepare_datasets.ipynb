{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of Datasets\n",
    "Datasets need to be first downloaded and copied in the folder \"raw_data\". The process is explained below for every single dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of BeRfiPl Dataset\n",
    "Please first download the datasets from the simulation ds1 at https://drive.google.com/drive/folders/1JJ80uSWyBDHvwBgc2UcIOlMgUsk7YhWt and save them in the folder raw_data/BeRfiPl/. <br>\n",
    "Done this, please execute the following lines of code. <br>\n",
    "The Code will save the preprocessed file in the directory preprocessed_data/BeRfiPl/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "!pip install openpyxl\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_BeRfiPl(raw_data_path, preprocessed_data_path):\n",
    "    df_n = pd.read_csv(raw_data_path, index_col=0)\n",
    "    # filter according to relecant columns\n",
    "    relevant_col_str_list = [\"time\", \"v_flow\", \"level\", \"m_flow\", \"fluidVolume\",\n",
    "                        \"N_in\", \"opening\", \"medium.t\", \"port_a.p\", \"port_b.p\"]\n",
    "    # list of columns that hold one of the string in the list above\n",
    "    col_selection = [str(c) for c in df_n.columns if any([e in c for e in relevant_col_str_list])]\n",
    "    #remove those that start with \"der(\"\n",
    "    col_selection = [c for c in col_selection if not c.startswith('der(')]\n",
    "    df_preprocessed = df_n.loc[:, col_selection].reset_index(drop=True)\n",
    "    df_preprocessed.to_csv(f'{preprocessed_data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeRfiPl_raw_anom_data_path = 'raw_data/BeRfiPl/ds1c.csv'\n",
    "BeRfiPl_raw_norm_data_path = 'raw_data/BeRfiPl/ds1n.csv'\n",
    "BeRfiPl_anom_data_path = 'preprocessed_data/BeRfiPl/ds1c.csv'\n",
    "BeRfiPl_norm_data_path = 'preprocessed_data/BeRfiPl/ds1n.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_BeRfiPl(BeRfiPl_raw_anom_data_path, BeRfiPl_anom_data_path)\n",
    "prepare_BeRfiPl(BeRfiPl_raw_norm_data_path, BeRfiPl_norm_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of SmA Dataset\n",
    "Please first download the dataset from https://github.com/thomasbierweiler/FaultsOf4-TankBatchProcess/blob/main/SmA-Four-Tank-Batch-Process_V2.zip and save them in the folder raw_data/SmA/. <br>\n",
    "Done this, please execute the following lines of code. <br>\n",
    "The Code will save the preprocessed files the directory preprocessed_data/SmA/.  <br>\n",
    "The first file will be exclusively from Deviation ID1. <br>\n",
    "The following files are a merged combination of ID1 (nominal behavior) and a anomaly Deviation ID (ID2 - ID10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_SmA(raw_data_path, preprocessed_data_path):\n",
    "    df = pd.read_csv(raw_data_path, delimiter=';', index_col=0)\n",
    "    df_norm =  df.loc[df[df.columns[0]] == 1].reset_index(drop=True).drop(columns=['DeviationID ValueY'])\n",
    "    df_norm.to_csv(f'{preprocessed_data_path}id1_norm.csv', index=False)\n",
    "    for i in range(9):\n",
    "        df_anomaly = df.loc[df[df.columns[0]] == i+2].reset_index(drop=True).drop(columns=['DeviationID ValueY'])\n",
    "        df_preprocessed = pd.concat([df_norm, df_anomaly])\n",
    "        df_preprocessed.to_csv(f'{preprocessed_data_path}id{i+2}_anomaly.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SmA_raw_data_path = 'raw_data/SmA/SmA-Four-Tank-Batch-Process_V2.csv'\n",
    "SmA_data_path = 'preprocessed_data/SmA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_SmA(SmA_raw_data_path, SmA_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of SWaT Dataset\n",
    "Please first request access from https://itrust.sutd.edu.sg/testbeds/secure-water-treatment-swat/ and download the datasets from directory SWaT/SWat.A1&A2_Dec2015/Physical via the provided link to a shared drive and save them in the folder raw_data/SWaT/. <br>\n",
    "Done this, please execute the following lines of code. <br>\n",
    "The Code will save the preprocessed files as a part of the whole with 60.000 samples to the directory preprocessed_data/SWaT/.  <br>\n",
    "The file will filter the data on the Process P1 and exclude the others. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_SWaT(raw_data_path, preprocessed_data_path):\n",
    "    df = pd.read_excel(raw_data_path, index_col=0, header=1)\n",
    "    df_int = df.select_dtypes(include=['float64', 'int64', 'object']).iloc[110000:170000]\n",
    "    der_cols = [c for c in df_int.columns if \"101\" in c]\n",
    "    df_p1 = df_int.loc[:, der_cols].iloc[:, :]\n",
    "    df_p1.to_csv(f'{preprocessed_data_path}', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWat_raw_anom_data_path = 'raw_data/SWaT/SWaT_Dataset_Attack_v0.xlsx'\n",
    "SWaT_raw_norm_data_path = 'raw_data/SWaT/SWaT_Dataset_Normal_v0.xlsx'\n",
    "SWaT_anom_data_path = 'preprocessed_data/SWaT/swat_anom_p1.csv'\n",
    "SWaT_norm_data_path = 'preprocessed_data/SWaT/swat_norm_p1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_SWaT(SWaT_raw_norm_data_path, SWaT_norm_data_path)\n",
    "prepare_SWaT(SWat_raw_anom_data_path, SWaT_anom_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
